{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg\n",
      "Obtaining file:///home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: src\n",
      "  Attempting uninstall: src\n",
      "    Found existing installation: src 0.1.0\n",
      "    Uninstalling src-0.1.0:\n",
      "      Successfully uninstalled src-0.1.0\n",
      "  Running setup.py develop for src\n",
      "Successfully installed src-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from src.models.dense import NN0\n",
    "from src.models.dense import NN1\n",
    "from src.models.gcn import GCN0\n",
    "\n",
    "from src.models.train_model import train_with_loss\n",
    "from src.models.train_model import random_splits\n",
    "\n",
    "from src.models.reg import make_preg_loss\n",
    "\n",
    "from src.models.reg import compute_a_hat\n",
    "\n",
    "from src.models.evaluate_model import evaluate0\n",
    "from src.models.evaluate_model import evaluate1\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from src.models.evaluate_model import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "train size: 350\n",
      "valid size: 140\n",
      "test size: 2218\n",
      "-------------------------------------------------------------\n",
      "mu: 0.0, train_acc: 0.9914, val_acc: 0.7929, test_acc: 0.8350\n",
      "-------------------------------------------------------------\n",
      "mu: 0.1, train_acc: 0.9886, val_acc: 0.8000, test_acc: 0.8399\n",
      "-------------------------------------------------------------\n",
      "mu: 0.2, train_acc: 0.9857, val_acc: 0.8000, test_acc: 0.8431\n",
      "-------------------------------------------------------------\n",
      "mu: 0.3, train_acc: 0.9857, val_acc: 0.8000, test_acc: 0.8490\n",
      "-------------------------------------------------------------\n",
      "mu: 0.4, train_acc: 0.9829, val_acc: 0.8000, test_acc: 0.8521\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m loss_fn \u001b[39m=\u001b[39m make_preg_loss(L_cls, L_preg, mu, A_hat)\n\u001b[1;32m     47\u001b[0m gcn_model \u001b[39m=\u001b[39m GCN0(num_node_features\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mnum_node_features,\n\u001b[1;32m     48\u001b[0m                  num_classes\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mnum_classes) \\\n\u001b[1;32m     49\u001b[0m                 \u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 51\u001b[0m gcn_model \u001b[39m=\u001b[39m train_with_loss(gcn_model, data, loss_fn, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m350\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m \u001b[39m# gcn_model = train2(gcn_model, data)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m acc \u001b[39m=\u001b[39m evaluate0(gcn_model, data)\n",
      "File \u001b[0;32m~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/train_model.py:99\u001b[0m, in \u001b[0;36mtrain_with_loss\u001b[0;34m(model, data, loss_fn, lr, weight_decay, num_epochs, beta)\u001b[0m\n\u001b[1;32m     <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/train_model.py?line=94'>95</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/train_model.py?line=96'>97</a>\u001b[0m Z \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/train_model.py?line=98'>99</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(data, Z)\n\u001b[1;32m    <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/train_model.py?line=99'>100</a>\u001b[0m \u001b[39m# print(loss)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/train_model.py?line=100'>101</a>\u001b[0m \u001b[39m# print('@@@@@@@@@@@@@@@@@@@')\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/train_model.py?line=102'>103</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/reg.py:18\u001b[0m, in \u001b[0;36mmake_preg_loss.<locals>.l\u001b[0;34m(data, Z)\u001b[0m\n\u001b[1;32m     <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/reg.py?line=14'>15</a>\u001b[0m M \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mtrain_mask\u001b[39m.\u001b[39msum()\n\u001b[1;32m     <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/reg.py?line=15'>16</a>\u001b[0m N \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mtrain_mask\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/reg.py?line=17'>18</a>\u001b[0m l_cls \u001b[39m=\u001b[39m L_cls(P[data\u001b[39m.\u001b[39;49mtrain_mask], Y[data\u001b[39m.\u001b[39mtrain_mask])\n\u001b[1;32m     <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/reg.py?line=18'>19</a>\u001b[0m l_preg \u001b[39m=\u001b[39m L_preg(P[data\u001b[39m.\u001b[39mreg_mask], Q[data\u001b[39m.\u001b[39mreg_mask])\n\u001b[1;32m     <a href='file:///~/dtu/2021-22-spring/advanced_machine_learning/p-reg/src/models/reg.py?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m M \u001b[39m*\u001b[39m l_cls \u001b[39m+\u001b[39m mu \u001b[39m/\u001b[39m N \u001b[39m*\u001b[39m l_preg\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=T.NormalizeFeatures())\n",
    "# no normalization, using n != 0 and m!= 0 instead\n",
    "# doesnt work, just use normalization instead\n",
    "\n",
    "# try with no normalization, using n != 0 and m!= 0 \n",
    "# dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "\n",
    "\n",
    "def set_reg_mask(data, A):\n",
    "    reg_indeces = random.sample(range(data.x.shape[0]), k=A)\n",
    "    reg_mask = torch.zeros(len(data.y), dtype=torch.bool)\n",
    "    for i in reg_indeces:\n",
    "        reg_mask[i] = True\n",
    "    data.reg_mask = reg_mask\n",
    "    return data\n",
    "\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "data = random_splits(data, 50, 20)\n",
    "# data = set_reg_mask(data, data.train_mask.shape[0])\n",
    "data.reg_mask = torch.ones_like(data.train_mask, dtype=torch.bool)\n",
    "L_cls = lambda x, y: F.nll_loss(torch.log(x), y, reduction='sum')\n",
    "L_preg = lambda x, y: F.cross_entropy(x, y, reduction='sum')\n",
    "\n",
    "A_hat = compute_a_hat(data)\n",
    "\n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "print(f'train size: {data.train_mask.sum()}')\n",
    "print(f'valid size: {data.valid_mask.sum()}')\n",
    "print(f'test size: {data.test_mask.sum()}')\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for mu in range(21):\n",
    "    mu = mu / 10\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "    loss_fn = make_preg_loss(L_cls, L_preg, mu, A_hat)\n",
    "\n",
    "    gcn_model = GCN0(num_node_features=dataset.num_node_features,\n",
    "                     num_classes=dataset.num_classes) \\\n",
    "                    .to(device)\n",
    "\n",
    "    gcn_model = train_with_loss(gcn_model, data, loss_fn, num_epochs=350)\n",
    "    # gcn_model = train2(gcn_model, data)\n",
    "    acc = evaluate0(gcn_model, data)\n",
    "    # print(f'mu: {mu}, reg, Accuracy: {acc:.4f}')\n",
    "\n",
    "    train_acc, val_acc, test_acc = evaluate1(gcn_model, data)\n",
    "    metrics.append({'mu': mu, 'train_acc': train_acc, 'val_acc': val_acc, 'test_acc': test_acc})\n",
    "    print(f'mu: {mu}, train_acc: {train_acc:.4f}, val_acc: {val_acc:.4f}, test_acc: {test_acc:.4f}')\n",
    "    print('-------------------------------------------------------------')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6428)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_preg = lambda x, y: F.cross_entropy(x, y)\n",
    "\n",
    "x = torch.tensor([\n",
    "    [0,1,1,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "y = torch.tensor([\n",
    "    [0,1,1,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "L_preg(x, y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19b21e06a3ecf92dacb2d0dce038f81bb705746e8008c815c25552dc2d0953db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('p-reg-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
