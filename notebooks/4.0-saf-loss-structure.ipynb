{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandor/dtu/2021-22-spring/advanced_machine_learning\n",
      "Obtaining file:///home/sandor/dtu/2021-22-spring/advanced_machine_learning\n",
      "\u001b[31mERROR: file:///home/sandor/dtu/2021-22-spring/advanced_machine_learning does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from src.models.dense import NN0\n",
    "from src.models.dense import NN1\n",
    "from src.models.gcn import GCN0\n",
    "\n",
    "from src.models.train_model import train\n",
    "from src.models.train_model import train_conf_pen\n",
    "from src.models.train_model import random_splits\n",
    "\n",
    "from src.models.evaluate_model import evaluate0\n",
    "from src.models.evaluate_model import evaluate1\n",
    "\n",
    "from src.models.reg import reg_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from src.models.evaluate_model import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preg_loss(L_cls, L_preg, mu, A_hat):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, pred):\n",
    "        L_cls = L_cls(pred[data.train_mask], data.y[data.train_mask])\n",
    "        L_preg = L_preg(pred[data.train_mask], A_hat@pred[data.reg_mask])\n",
    "        return  + mu * L_preg(x,y)\n",
    "    return l\n",
    "\n",
    "\n",
    "def make_confidence_penalty_loss(L_cls, beta):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, pred):\n",
    "        L_cls = L_cls(pred[data.train_mask], data.y[data.train_mask])\n",
    "        H = -torch.sum(pred[data.reg_mask] * torch.log2(pred[data.reg_mask])) / pred.shape[0]\n",
    "\n",
    "        return L_cls - beta * H\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_cls = lambda x, y: F.nll_loss(torch.log(x), y),\n",
    "L_preg = lambda x, y: F.cross_entropy(x, y),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=T.NormalizeFeatures())\n",
    "# no normalization, using n != 0 and m!= 0 instead\n",
    "# doesnt work, just use normalization instead\n",
    "# dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "data = random_splits(data, 50, 20)\n",
    "splits = data.train_mask, data.val_mask, data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20)\n"
     ]
    }
   ],
   "source": [
    "def set_reg_mask(data, A):\n",
    "    reg_indeces = random.sample(range(data.x.shape[0]), k=A)\n",
    "    reg_mask = torch.zeros(len(data.y), dtype=torch.bool)\n",
    "    for i in reg_indeces:\n",
    "        reg_mask[i] = True\n",
    "    data.reg_mask = reg_mask\n",
    "    return data\n",
    "\n",
    "data = set_reg_mask(data, 20)\n",
    "print(data.reg_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3356, grad_fn=<AddBackward0>)\n",
      "tensor(2.3356, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10823/777732340.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  l_cls = L_cls(F.softmax(pred[data.train_mask]), data.y[data.train_mask])\n",
      "/tmp/ipykernel_10823/777732340.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  l_preg = L_preg(F.softmax(pred[data.reg_mask]), F.softmax((A_hat@pred)[data.reg_mask]))\n"
     ]
    }
   ],
   "source": [
    "from src.models.reg import compute_a_hat\n",
    "\n",
    "def make_preg_loss(L_cls, L_preg, mu, A_hat):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, Z):\n",
    "        P = F.softmax(Z[data.train_mask], dim=1)\n",
    "        Q = F.softmax((A_hat@Z)[data.train_mask], dim=1)\n",
    "        Y = data.y[data.train_mask]\n",
    "        M = data.train_mask.sum()\n",
    "        N = data.train_mask.shape[0]\n",
    "\n",
    "        l_cls = L_cls(P, Y)\n",
    "        l_preg = L_preg(P, Q)\n",
    "\n",
    "        return - 1 / M * l_cls + mu / N * l_preg\n",
    "    return l\n",
    "\n",
    "\n",
    "def make_confidence_penalty_loss(L_cls, beta):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, pred):\n",
    "        l_cls = L_cls(pred[data.train_mask], data.y[data.train_mask])\n",
    "        H = -torch.sum(pred[data.reg_mask] * torch.log2(pred[data.reg_mask])) / pred.shape[0]\n",
    "\n",
    "        return l_cls - beta * H\n",
    "    return l\n",
    "\n",
    "\n",
    "a_hat = compute_a_hat(data)\n",
    "\n",
    "L_cls = lambda x, y: F.nll_loss(torch.log(x), y)\n",
    "L_preg = lambda x, y: F.cross_entropy(x, y)\n",
    "\n",
    "l = make_preg_loss(L_cls, L_preg, 0.2, a_hat)\n",
    "\n",
    "\n",
    "gcn_model = GCN0(num_node_features=dataset.num_node_features,\n",
    "                num_classes=dataset.num_classes) \\\n",
    "                .to(device)\n",
    "\n",
    "pred = gcn_model(data)\n",
    "comped_l = l(data, pred)\n",
    "comped_l_old = reg_loss(\n",
    "            pred=F.softmax(pred, dim=1),\n",
    "            prop=F.softmax(a_hat@pred, dim=1), # \n",
    "            target=data.y[data.train_mask], # we assume to have ground-truth labels on only a subset of the dataset\n",
    "            train_mask=data.train_mask, # hence, we also provide the train_mask, to know what nodes have labels\n",
    "            preg_mask=torch.ones_like(data.train_mask, dtype=torch.bool),\n",
    "            L_cls=lambda x, y: F.nll_loss(torch.log(x), y),\n",
    "            L_preg=lambda x, y: F.cross_entropy(x, y),\n",
    "            mu=0.2,\n",
    "            )\n",
    "print(comped_l)\n",
    "print(comped_l_old)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19b21e06a3ecf92dacb2d0dce038f81bb705746e8008c815c25552dc2d0953db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('p-reg-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
