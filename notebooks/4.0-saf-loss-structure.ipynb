{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from src.models.dense import NN0\n",
    "from src.models.dense import NN1\n",
    "from src.models.gcn import GCN0\n",
    "\n",
    "from src.models.train_model import train\n",
    "from src.models.train_model import train_conf_pen\n",
    "from src.models.train_model import random_splits\n",
    "\n",
    "from src.models.evaluate_model import evaluate0\n",
    "from src.models.evaluate_model import evaluate1\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from src.models.evaluate_model import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preg_loss(L_cls, L_preg, mu, A_hat):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, pred):\n",
    "        L_cls = L_cls(pred[data.train_mask], data.y[data.train_mask])\n",
    "        L_preg = L_preg(pred[data.train_mask], A_hat@pred[data.reg_mask])\n",
    "        return  + mu * L_preg(x,y)\n",
    "    return l\n",
    "\n",
    "\n",
    "def make_confidence_penalty_loss(L_cls, beta):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, pred):\n",
    "        L_cls = L_cls(pred[data.train_mask], data.y[data.train_mask])\n",
    "        H = -torch.sum(pred[data.reg_mask] * torch.log2(pred[data.reg_mask])) / pred.shape[0]\n",
    "\n",
    "        return L_cls - beta * H\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_cls = lambda x, y: F.nll_loss(torch.log(x), y),\n",
    "L_preg = lambda x, y: F.cross_entropy(x, y),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_preg_loss() missing 1 required positional argument: 'A_hat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg/notebooks/4.0-saf-loss-structure.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg/notebooks/4.0-saf-loss-structure.ipynb#ch0000004?line=0'>1</a>\u001b[0m l \u001b[39m=\u001b[39m make_preg_loss(L_cls, L_preg, \u001b[39m0.2\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: make_preg_loss() missing 1 required positional argument: 'A_hat'"
     ]
    }
   ],
   "source": [
    "l = make_preg_loss(L_cls, L_preg, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=T.NormalizeFeatures())\n",
    "# no normalization, using n != 0 and m!= 0 instead\n",
    "# doesnt work, just use normalization instead\n",
    "# dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "data = random_splits(data, 50, 20)\n",
    "splits = data.train_mask, data.val_mask, data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20)\n"
     ]
    }
   ],
   "source": [
    "def set_reg_mask(data, A):\n",
    "    reg_indeces = random.sample(range(data.x.shape[0]), k=A)\n",
    "    reg_mask = torch.zeros(len(data.y), dtype=torch.bool)\n",
    "    for i in reg_indeces:\n",
    "        reg_mask[i] = True\n",
    "    data.reg_mask = reg_mask\n",
    "    return data\n",
    "\n",
    "data = set_reg_mask(data, 20)\n",
    "print(data.reg_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from src.models.reg import compute_a_hat\n",
    "\n",
    "def make_preg_loss(L_cls, L_preg, mu, A_hat):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, pred):\n",
    "        l_cls = L_cls(pred[data.train_mask], data.y[data.train_mask])\n",
    "        l_preg = L_preg(pred[data.reg_mask], (A_hat@pred)[data.reg_mask])\n",
    "        return l_cls + mu * l_preg\n",
    "    return l\n",
    "\n",
    "\n",
    "def make_confidence_penalty_loss(L_cls, beta):\n",
    "    \"\"\"  \"\"\"\n",
    "    def l(data, pred):\n",
    "        l_cls = L_cls(pred[data.train_mask], data.y[data.train_mask])\n",
    "        H = -torch.sum(pred[data.reg_mask] * torch.log2(pred[data.reg_mask])) / pred.shape[0]\n",
    "\n",
    "        return l_cls - beta * H\n",
    "    return l\n",
    "\n",
    "\n",
    "a_hat = compute_a_hat(data)\n",
    "\n",
    "L_cls = lambda x, y: F.nll_loss(torch.log(x), y)\n",
    "L_preg = lambda x, y: F.cross_entropy(x, y)\n",
    "\n",
    "l = make_preg_loss(L_cls, L_preg, 0.2, a_hat)\n",
    "\n",
    "\n",
    "gcn_model = GCN0(num_node_features=dataset.num_node_features,\n",
    "                num_classes=dataset.num_classes) \\\n",
    "                .to(device)\n",
    "\n",
    "pred = gcn_model(data)\n",
    "comped_l = l(data, pred)\n",
    "print(comped_l)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19b21e06a3ecf92dacb2d0dce038f81bb705746e8008c815c25552dc2d0953db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('p-reg-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
