{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg\n",
      "Obtaining file:///home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: src\n",
      "  Attempting uninstall: src\n",
      "    Found existing installation: src 0.1.0\n",
      "    Uninstalling src-0.1.0:\n",
      "      Successfully uninstalled src-0.1.0\n",
      "  Running setup.py develop for src\n",
      "Successfully installed src-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models.train_model import train_with_loss\n",
    "from src.models.train_model import random_splits\n",
    "\n",
    "from src.models.gcn import GCN_fix_2layer\n",
    "\n",
    "from src.models.reg import make_preg_ce_ce\n",
    "from src.models.reg import make_preg_ce_ce_alt\n",
    "from src.models.reg import make_abduls_preg_loss\n",
    "from src.models.reg import make_lap_loss_ce\n",
    "from src.models.reg import compute_a_hat\n",
    "\n",
    "from src.models.evaluate_model import evaluate1\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=T.NormalizeFeatures())\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "A_hat = compute_a_hat(data)\n",
    "\n",
    "# data = random_splits(data, 50, 20)\n",
    "data.reg_mask = torch.ones_like(data.train_mask, dtype=torch.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "train size: 140\n",
      "val size: 500\n",
      "test size: 1000\n",
      "-------------------------------------------------------------\n",
      "{'loss_fn_factory': 'cece', 'mu': 0, 'train_acc': 1.0, 'val_acc': 0.784, 'test_acc': 0.8}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.1, 'train_acc': 1.0, 'val_acc': 0.794, 'test_acc': 0.817}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.2, 'train_acc': 0.9929, 'val_acc': 0.802, 'test_acc': 0.822}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.3, 'train_acc': 0.9929, 'val_acc': 0.802, 'test_acc': 0.821}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.4, 'train_acc': 0.9929, 'val_acc': 0.806, 'test_acc': 0.819}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.5, 'train_acc': 0.9929, 'val_acc': 0.806, 'test_acc': 0.821}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.6, 'train_acc': 0.9929, 'val_acc': 0.802, 'test_acc': 0.821}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.7, 'train_acc': 0.9929, 'val_acc': 0.722, 'test_acc': 0.768}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.8, 'train_acc': 0.9571, 'val_acc': 0.518, 'test_acc': 0.521}\n",
      "{'loss_fn_factory': 'cece', 'mu': 0.9, 'train_acc': 0.9286, 'val_acc': 0.466, 'test_acc': 0.463}\n",
      "{'loss_fn_factory': 'cece', 'mu': 1.0, 'train_acc': 0.8929, 'val_acc': 0.41, 'test_acc': 0.411}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0, 'train_acc': 1.0, 'val_acc': 0.796, 'test_acc': 0.803}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.1, 'train_acc': 1.0, 'val_acc': 0.788, 'test_acc': 0.815}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.2, 'train_acc': 0.9929, 'val_acc': 0.804, 'test_acc': 0.829}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.3, 'train_acc': 0.9929, 'val_acc': 0.796, 'test_acc': 0.824}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.4, 'train_acc': 1.0, 'val_acc': 0.806, 'test_acc': 0.822}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.5, 'train_acc': 0.9929, 'val_acc': 0.794, 'test_acc': 0.82}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.6, 'train_acc': 0.9786, 'val_acc': 0.804, 'test_acc': 0.824}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.7, 'train_acc': 0.9786, 'val_acc': 0.798, 'test_acc': 0.818}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.8, 'train_acc': 0.9786, 'val_acc': 0.768, 'test_acc': 0.799}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 0.9, 'train_acc': 0.95, 'val_acc': 0.648, 'test_acc': 0.651}\n",
      "{'loss_fn_factory': 'cecealt', 'mu': 1.0, 'train_acc': 0.9071, 'val_acc': 0.536, 'test_acc': 0.542}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0, 'train_acc': 1.0, 'val_acc': 0.788, 'test_acc': 0.807}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.1, 'train_acc': 0.9929, 'val_acc': 0.804, 'test_acc': 0.817}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.2, 'train_acc': 1.0, 'val_acc': 0.806, 'test_acc': 0.815}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.3, 'train_acc': 0.9929, 'val_acc': 0.816, 'test_acc': 0.824}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.4, 'train_acc': 1.0, 'val_acc': 0.802, 'test_acc': 0.824}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.5, 'train_acc': 0.9786, 'val_acc': 0.794, 'test_acc': 0.827}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.6, 'train_acc': 0.9714, 'val_acc': 0.798, 'test_acc': 0.831}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.7, 'train_acc': 0.9786, 'val_acc': 0.752, 'test_acc': 0.766}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.8, 'train_acc': 0.9857, 'val_acc': 0.718, 'test_acc': 0.738}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 0.9, 'train_acc': 0.9857, 'val_acc': 0.75, 'test_acc': 0.757}\n",
      "{'loss_fn_factory': 'abduls', 'mu': 1.0, 'train_acc': 0.9357, 'val_acc': 0.472, 'test_acc': 0.491}\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print(f'train size: {data.train_mask.sum()}')\n",
    "print(f'val size: {data.val_mask.sum()}')\n",
    "print(f'test size: {data.test_mask.sum()}')\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for mu in [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.]:\n",
    "    torch.manual_seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "    loss_fn = make_preg_ce_ce(mu, A_hat)\n",
    "    \n",
    "    model = GCN_fix_2layer()\n",
    "\n",
    "    model = train_with_loss(model, data, loss_fn, num_epochs=200)\n",
    "\n",
    "    train_acc, val_acc, test_acc = evaluate1(model, data)\n",
    "    metrics.append({'loss_fn_factory': 'cece', 'mu': mu, 'train_acc': np.round(train_acc,4), 'val_acc': np.round(val_acc,4), 'test_acc': np.round(test_acc,4)})\n",
    "    print(metrics[-1])\n",
    "\n",
    "for mu in [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.]:\n",
    "    # torch.manual_seed(1)\n",
    "    # random.seed(1)\n",
    "\n",
    "    loss_fn = make_preg_ce_ce_alt(mu, A_hat)\n",
    "    \n",
    "    model = GCN_fix_2layer()\n",
    "\n",
    "    model = train_with_loss(model, data, loss_fn, num_epochs=200)\n",
    "\n",
    "    train_acc, val_acc, test_acc = evaluate1(model, data)\n",
    "    metrics.append({'loss_fn_factory': 'cecealt', 'mu': mu, 'train_acc': np.round(train_acc,4), 'val_acc': np.round(val_acc,4), 'test_acc': np.round(test_acc,4)})\n",
    "    print(metrics[-1])\n",
    "\n",
    "for mu in [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.]:\n",
    "    # torch.manual_seed(1)\n",
    "    # random.seed(1)\n",
    "\n",
    "    loss_fn = make_abduls_preg_loss(mu, A_hat)\n",
    "    \n",
    "    model = GCN_fix_2layer()\n",
    "\n",
    "    model = train_with_loss(model, data, loss_fn, num_epochs=200)\n",
    "\n",
    "    train_acc, val_acc, test_acc = evaluate1(model, data)\n",
    "    metrics.append({'loss_fn_factory': 'abduls', 'mu': mu, 'train_acc': np.round(train_acc,4), 'val_acc': np.round(val_acc,4), 'test_acc': np.round(test_acc,4)})\n",
    "    print(metrics[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_fn_factory': 'lap', 'mu': 0, 'train_acc': 1.0, 'val_acc': 0.784, 'test_acc': 0.8}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.1, 'train_acc': 0.9857, 'val_acc': 0.628, 'test_acc': 0.649}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.2, 'train_acc': 0.9857, 'val_acc': 0.586, 'test_acc': 0.585}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.3, 'train_acc': 0.9857, 'val_acc': 0.586, 'test_acc': 0.58}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.4, 'train_acc': 0.9786, 'val_acc': 0.586, 'test_acc': 0.573}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.5, 'train_acc': 0.9786, 'val_acc': 0.586, 'test_acc': 0.572}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.6, 'train_acc': 0.9786, 'val_acc': 0.578, 'test_acc': 0.577}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.7, 'train_acc': 0.9786, 'val_acc': 0.572, 'test_acc': 0.577}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.8, 'train_acc': 0.9786, 'val_acc': 0.578, 'test_acc': 0.586}\n",
      "{'loss_fn_factory': 'lap', 'mu': 0.9, 'train_acc': 0.9786, 'val_acc': 0.576, 'test_acc': 0.585}\n",
      "{'loss_fn_factory': 'lap', 'mu': 1.0, 'train_acc': 0.9786, 'val_acc': 0.576, 'test_acc': 0.577}\n"
     ]
    }
   ],
   "source": [
    "for mu in [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.]:\n",
    "    torch.manual_seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "    loss_fn = make_lap_loss_ce(mu)\n",
    "    \n",
    "    model = GCN_fix_2layer()\n",
    "\n",
    "    model = train_with_loss(model, data, loss_fn, num_epochs=200)\n",
    "\n",
    "    train_acc, val_acc, test_acc = evaluate1(model, data)\n",
    "    metrics.append({'loss_fn_factory': 'lap', 'mu': mu, 'train_acc': np.round(train_acc,4), 'val_acc': np.round(val_acc,4), 'test_acc': np.round(test_acc,4)})\n",
    "    print(metrics[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metrics)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19b21e06a3ecf92dacb2d0dce038f81bb705746e8008c815c25552dc2d0953db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('p-reg-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
