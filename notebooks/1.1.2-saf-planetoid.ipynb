{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg\n",
      "Obtaining file:///home/sandor/dtu/2021-22-spring/advanced_machine_learning/p-reg\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: src\n",
      "  Attempting uninstall: src\n",
      "    Found existing installation: src 0.1.0\n",
      "    Uninstalling src-0.1.0:\n",
      "      Successfully uninstalled src-0.1.0\n",
      "  Running setup.py develop for src\n",
      "Successfully installed src-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.dense import NN0\n",
    "from src.models.dense import NN1\n",
    "from src.models.gcn import GCN0\n",
    "\n",
    "from src.models.train_model import train0\n",
    "from src.models.train_model import train1\n",
    "from src.models.train_model import random_splits\n",
    "\n",
    "from src.models.evaluate_model import evaluate0\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triaing the model\n",
    "gcn_model = GCN0(num_node_features=dataset.num_node_features,\n",
    "                    num_classes=dataset.num_classes) \\\n",
    "                    .to(device)\n",
    "\n",
    "gcn_model = train0(gcn_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[data.train_mask].numpy().tolist().count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 7])\n",
      "L_reg:\t0.012982075102627277\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def reg_loss(p, t, train_mask, A_hat, mu=0.2, phi='ce'):\n",
    "    \"\"\"\n",
    "    Regularization loss\n",
    "    float tensor[N,C] p: predictions on the entire dataset\n",
    "    int tensor[M] t: list of ground-truth class indeces for the training nodes\n",
    "    bool tensor[N] train_mask: bool map for selecting the training nodes from the entire dataset\n",
    "    bool tensor A_hat[N, N]: adjacency matrix for the entire dataset\n",
    "    float mu: regularization factor\n",
    "    \"\"\"\n",
    "\n",
    "    L_1 = F.cross_entropy(p[train_mask], t)\n",
    "\n",
    "    if phi == 'ce':\n",
    "        L_2 = F.cross_entropy(p, A_hat@p)\n",
    "    elif phi == 'l2':\n",
    "        L_2 = F.mse_loss(p, A_hat@p)\n",
    "    elif phi == 'kld':\n",
    "        L_2 = F.kl_div(p, A_hat@p)\n",
    "    else:\n",
    "        raise ValueError('phi must be one of ce (cross_entropy), l2 (squared error) or kld (kullback-leibler divergence)')\n",
    "\n",
    "    \n",
    "    M = train_mask.sum()\n",
    "    N = train_mask.shape[0]\n",
    "\n",
    "    return 1 / M * L_1 + mu / N * L_2\n",
    "\n",
    "gcn_model = GCN0(num_node_features=dataset.num_node_features,\n",
    "                 num_classes=dataset.num_classes) \\\n",
    "                .to(device)\n",
    "\n",
    "\n",
    "def compute_a(data):\n",
    "    a = torch.zeros(data.x.shape[0], data.x.shape[0])\n",
    "\n",
    "    for i in data.edge_index.T:\n",
    "        a[i[0], i[1]] = 1\n",
    "\n",
    "    return a\n",
    "\n",
    "A = compute_a(data)\n",
    "D = torch.diag(A.sum(dim=0))\n",
    "A_hat = D.inverse()@A\n",
    "\n",
    "out = gcn_model(data)\n",
    "print(out[data.train_mask].shape)\n",
    "\n",
    "loss = reg_loss(\n",
    "        p=out, # we make predictions on the entire dataset\n",
    "        t=data.y[data.train_mask], # we assume to have ground-truth labels on only a subset of the dataset\n",
    "        train_mask=data.train_mask, # hence, we also provide the train_mask, to know what nodes have labels\n",
    "        A_hat=A_hat, \n",
    "        mu=0.1)\n",
    "\n",
    "print(f'L_reg:\\t{loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1461,  1.0924,  1.0553,  1.3307,  0.1736],\n",
      "        [-0.8848,  1.7084,  0.6767,  0.6350,  1.3613],\n",
      "        [ 0.4374, -0.5261, -2.4688,  1.1065, -0.7168]], requires_grad=True)\n",
      "tensor([2, 1, 4])\n",
      "tensor(1.6512, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n.backward()\\n# Example of target with class probabilities\\ninput = torch.randn(3, 5, requires_grad=True)\\ntarget = torch.randn(3, 5).softmax(dim=1)\\nloss = F.cross_entropy(input, target)\\nloss.backward()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "print(input)\n",
    "print(target)\n",
    "print(loss)\n",
    "'''\n",
    ".backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19b21e06a3ecf92dacb2d0dce038f81bb705746e8008c815c25552dc2d0953db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('p-reg-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
